from typing import List, Union
from utils import get_examples, make_context, get_response
from tools import LLM, Tools
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

class Pipeline:
    def __init__(
        self,
        llm_agent: LLM,
        model_name: str,
        sample_prompt: str,
        respond_prompt: str,
        search_tool: Tools,
        num_retrieved_pages: int
    ):
        """
        Initializes the Pipeline with the necessary components.

        Parameters:
            llm_agent (LLM): The language model agent to use for generating and responding.
            model_name (str): The name of the model to use.
            sample_prompt (str): The prompt used to generate sample questions.
            respond_prompt (str): The prompt used for generating responses to questions.
            search_tool (Tools): The search tool to use for retrieving context.
            num_retrieved_pages (int): The number of pages to retrieve for context.
        """
        self.llm = llm_agent
        self.model_name = model_name
        self.sample_prompt = sample_prompt
        self.respond_prompt = respond_prompt
        self.search_tool = search_tool
        self.num_retrieved_pages = num_retrieved_pages

    def __call__(
        self,
        seed_questions: Union[str, List[str]],
        verbose: bool = False,
        seed_as_instructions: bool = False,
        sample_size: int = None,
        iterations: int = 1,
        max_workers: int = 4  # Added max_workers parameter
    ):
        """
        Executes the pipeline with the provided seed questions.

        Parameters:
            seed_questions (str or List[str]): The seed question(s) to start with.
            verbose (bool): If True, prints additional information during execution.
            seed_as_instructions (bool): If True, use seed questions as instructions.
            sample_size (int, optional): Number of seed questions to randomly select.
            iterations (int): Number of times to run the pipeline. Defaults to 1.
            max_workers (int): Maximum number of worker threads to use for processing questions.

        Returns:
            List[dict]: A list of instructions generated by the pipeline.
        """
        all_instructions = []

        # Add progress bar for iterations
        for iteration in tqdm(range(iterations), desc="Iterations", unit="iteration"):
            if verbose:
                print(f"\nStarting iteration {iteration + 1} of {iterations}")

            # Generate sample questions based on the seed questions
            samples = get_examples(
                seed=seed_questions,
                agent=self.llm,
                model=self.model_name,
                prompt=self.sample_prompt,
                verbose=verbose,
                seed_as_instructs=seed_as_instructions,
                sample_size=sample_size
            )

            if verbose:
                print(f"Generated Samples: {samples}")

            questions = samples['questions']

            # Prepare the progress bar for questions
            question_progress = tqdm(total=len(questions), desc="Questions", unit="question", leave=False)

            # Use ThreadPoolExecutor for concurrent processing of questions
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_question = {
                    executor.submit(self.process_question, question, verbose): question for question in questions
                }

                for future in as_completed(future_to_question):
                    question = future_to_question[future]
                    try:
                        instruction = future.result()
                        if instruction:
                            all_instructions.append(instruction)
                    except Exception as err:
                        print(f"Error processing question '{question}': {err}")
                    finally:
                        question_progress.update(1)

            question_progress.close()

        return all_instructions

    def process_question(self, question: str, verbose: bool = False):
        """
        Processes a single question: retrieves context, generates response.

        Parameters:
            question (str): The question to process.
            verbose (bool): If True, prints additional information.

        Returns:
            dict: The instruction generated for the question.
        """
        try:
            # Generate context and retrieve links using the search tool
            context, links = make_context(
                query=question,
                tool=self.search_tool,
                num=self.num_retrieved_pages,
                verbose=verbose
            )
            # Truncate context if necessary
            max_context_length = 200000
            truncated_context = context[:min(max_context_length, len(context))]
            # Get the response using the LLM
            instruction = get_response(
                query=question,
                context=truncated_context,
                links=links,
                agent=self.llm,
                model=self.model_name,
                prompt=self.respond_prompt,
                verbose=verbose
            )
            return instruction
        except Exception as err:
            print(f"Unexpected error processing question '{question}': {err}, type: {type(err)}")
            return None
